{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7647ac1-b70e-476e-a421-627a468dde70",
   "metadata": {},
   "source": [
    "## Accelerating 2:4 sparse models with Huggingface, torch.compile, and semi-structured sparsity. \n",
    "\n",
    "This notebook shows how to accelerate an off-the-shelf 2:4 sparse model using pytorch's `to_sparse_semi_structured`\n",
    "\n",
    "It takes advantage of the model checkpoints offered by neuralmagic: https://huggingface.co/nm-testing/SparseLlama-3-8B-pruned_50.2of4-FP8\n",
    "\n",
    "\n",
    "Even though we need to pad the matmul shapes from (1, hidden) @ (hidden, output) to (8, hidden) @ (hidden, output) we are still able to achieve speedups on the mlp.up and mlp.gate linear layesr of the FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2600c92e-e096-4492-8188-4cff935d3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.sparse import to_sparse_semi_structured\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "\n",
    "def benchmark(fn, WARMUP=5, N=250):\n",
    "    time_per_batch = []\n",
    "    with torch.no_grad():\n",
    "        # warmup steps\n",
    "        for _ in range(WARMUP):\n",
    "            timed(fn)\n",
    "    \n",
    "        # benchmark\n",
    "        for _ in tqdm(range(N)):\n",
    "            with torch.no_grad():\n",
    "                _ , time_sec =  timed(fn)\n",
    "                time_per_batch.append(time_sec)\n",
    "            \n",
    "    # each time we generate 128 tokens - 7 for the prompt = 121 tokens at a time.\n",
    "    total_time = sum(time_per_batch)\n",
    "    tokens_per_second = 121 * N / total_time\n",
    "    #print(f\"Total time: {total_time:.3f}s | Tokens/second: {tokens_per_second:.3f}\")\n",
    "    print(total_time / N)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04cfed81-c63b-413e-b80f-e4eee983e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sparse.SparseSemiStructuredTensor._FORCE_CUTLASS = False\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # silence warnings when compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325f62a3-a62b-4ad9-b050-cf48bbc0c6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f363aad589144f39c569e57e2adb92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/sparse/semi_structured.py:113: UserWarning: The PyTorch API of SparseSemiStructuredTensor is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.sparse module for further information about the project.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.self_attn.q_proj torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj torch.Size([4096, 14336])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"nm-testing/SparseLlama-3-8B-pruned_50.2of4\", torch_dtype=torch.float16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nm-testing/SparseLlama-3-8B-pruned_50.2of4\")\n",
    "\n",
    "# Load semi-structured spares\n",
    "for name, mod in model.named_modules():\n",
    "    if isinstance(mod, torch.nn.Linear):\n",
    "\n",
    "        # print out linear layers just FYI\n",
    "        if '10' in name:\n",
    "            print(name, mod.weight.shape)\n",
    "\n",
    "        # these two will show speedups\n",
    "        if 'mlp.gate' in name or 'mlp.up' in name:    \n",
    "            mod.weight = torch.nn.Parameter(to_sparse_semi_structured(mod.weight))\n",
    "\n",
    "# Specify the max length (including both the prompt and the response)\n",
    "# When calling `generate` with `cache_implementation=\"static\" later, this is also used to create a `StaticCache` object\n",
    "# with sequence length = `max_length`. The longer the more you will re-use it\n",
    "model.generation_config.max_length = 128\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "\n",
    "prompt = \"Why dogs are so cute?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0356e3-0403-40e3-b41a-9d1fe18741c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without `torch.compile`: each call takes ~ 5.0 seconds (on A100 80G + torch 2.3)\n",
    "# Total time: 168.715s | Tokens/second: 17.930\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0504dad-7a44-4b95-9558-6d53c593da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `torch.compile(model, ...)` is not recommended as you compile callbacks\n",
    "# and full generate. We recommend compiling only the forward for now. \n",
    "# \"reduce-overhead\" will use cudagraphs.\n",
    "torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit = None\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bb32a-ec1a-4198-b59d-314288fc0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9b9b1-d098-45d4-88e0-c75ee56caf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check we get same output as non-compiled model\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ea567-b389-4253-aff3-0d7b3c98561e",
   "metadata": {},
   "source": [
    "## Run torch.compile baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654779b-cb75-4ebc-8022-c4df4a708489",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"nm-testing/SparseLlama-3-8B-pruned_50.2of4\", torch_dtype=torch.float16).cuda()\n",
    "\n",
    "# set configs again\n",
    "model.generation_config.max_length = 128\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "benchmark(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d033612-2653-4546-8d50-497912cb8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e88de6-989b-45bc-bfe3-43d7132ad7c9",
   "metadata": {},
   "source": [
    "# e2e runs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd907f7-6019-4b8b-be68-008b12d5456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/hf_eval.py --tasks hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e92dd-d457-4cb7-80d3-d7b023590901",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/hf_eval.py --sparsity semi_sparse_mlp_only --repo_id \"nm-testing/SparseLlama-3-8B-pruned_50.2of4\" --tasks hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f49163-52c3-4a4f-a4f3-db849e85dca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
