{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7647ac1-b70e-476e-a421-627a468dde70",
   "metadata": {},
   "source": [
    "## Accelerating 2:4 sparse models with Huggingface, torch.compile, and semi-structured sparsity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2600c92e-e096-4492-8188-4cff935d3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.sparse import to_sparse_semi_structured\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000\n",
    "\n",
    "\n",
    "def benchmark(model, WARMUP=5, N=25):\n",
    "    time_per_batch = []\n",
    "    with torch.no_grad():\n",
    "        # warmup steps\n",
    "        for _ in range(WARMUP):\n",
    "            timed(lambda: model.generate(**inputs))\n",
    "    \n",
    "        # benchmark\n",
    "        for _ in tqdm(range(N)):\n",
    "            with torch.no_grad():\n",
    "                _ , time_sec =  timed(lambda: model.generate(**inputs))\n",
    "                time_per_batch.append(time_sec)\n",
    "            \n",
    "    # each time we generate 128 tokens - 7 for the prompt = 121 tokens at a time.\n",
    "    total_time = sum(time_per_batch)\n",
    "    tokens_per_second = 121 * N / total_time\n",
    "    print(f\"Total time: {total_time:.3f}s | Tokens/second: {tokens_per_second:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04cfed81-c63b-413e-b80f-e4eee983e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sparse.SparseSemiStructuredTensor._FORCE_CUTLASS = False\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # silence warnings when compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "325f62a3-a62b-4ad9-b050-cf48bbc0c6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f968aa4752e2427e93717844400101cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/sparse/semi_structured.py:113: UserWarning: The PyTorch API of SparseSemiStructuredTensor is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.sparse module for further information about the project.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.10.self_attn.q_proj torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj torch.Size([4096, 14336])\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"nm-testing/SparseLlama-3-8B-pruned_50.2of4\", torch_dtype=torch.float16).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nm-testing/SparseLlama-3-8B-pruned_50.2of4\")\n",
    "\n",
    "# Load semi-structured spares\n",
    "for name, mod in model.named_modules():\n",
    "    if isinstance(mod, torch.nn.Linear):\n",
    "\n",
    "        # print out linear layers just FYI\n",
    "        if '10' in name:\n",
    "            print(name, mod.weight.shape)\n",
    "\n",
    "        # these two will show speedups\n",
    "        if 'mlp.gate' in name or 'mlp.up' in name:    \n",
    "            mod.weight = torch.nn.Parameter(to_sparse_semi_structured(mod.weight))\n",
    "\n",
    "# Specify the max length (including both the prompt and the response)\n",
    "# When calling `generate` with `cache_implementation=\"static\" later, this is also used to create a `StaticCache` object\n",
    "# with sequence length = `max_length`. The longer the more you will re-use it\n",
    "model.generation_config.max_length = 128\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "\n",
    "prompt = \"Why dogs are so cute?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c6d8bf-958c-4773-9a08-0f0e42786ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Why dogs are so cute? Why do we love them so much? Why do we want to hug them and kiss them and play with them? Why do we want to take care of them and make sure they are happy and healthy? The answer is simple: because they are cute! But what makes a dog cute? Is it their big eyes, their floppy ears, their wagging tail, or something else? In this article, we will explore the science behind why dogs are so cute and why we love them so much.\n",
      "\n",
      "First, let's talk about the big eyes. Dogs have large eyes that are set far apart on their\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# without `torch.compile`: each call takes ~ 5.0 seconds (on A100 80G + torch 2.3)\n",
    "# Total time: 168.715s | Tokens/second: 17.930\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)\n",
    "\n",
    "# `torch.compile(model, ...)` is not recommended as you compile callbacks\n",
    "# and full generate. We recommend compiling only the forward for now. \n",
    "# \"reduce-overhead\" will use cudagraphs.\n",
    "torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit = None\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86bb32a-ec1a-4198-b59d-314288fc0f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 25/25 [00:34<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 34.736s | Tokens/second: 87.086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b9b9b1-d098-45d4-88e0-c75ee56caf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Why dogs are so cute? Why do we love them so much? Why do we want to hug them and kiss them and play with them? Why do we want to take care of them and make sure they are happy and healthy? The answer is simple: because they are cute! But what makes a dog cute? Is it their big eyes, their floppy ears, their wagging tail, or something else? In this article, we will explore the science behind why dogs are so cute and why we love them so much.\n",
      "\n",
      "First, let's talk about the big eyes. Dogs have large eyes that are set far apart on their\n"
     ]
    }
   ],
   "source": [
    "# sanity check we get same output as non-compiled model\n",
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ea567-b389-4253-aff3-0d7b3c98561e",
   "metadata": {},
   "source": [
    "## Run torch.compile baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0654779b-cb75-4ebc-8022-c4df4a708489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff33b04af724f2e9cda510d72c88828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████| 25/25 [00:37<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 37.925s | Tokens/second: 79.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"nm-testing/SparseLlama-3-8B-pruned_50.2of4\", torch_dtype=torch.float16).cuda()\n",
    "\n",
    "# set configs again\n",
    "model.generation_config.max_length = 128\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "benchmark(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d033612-2653-4546-8d50-497912cb8c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Why dogs are so cute? Why do we love them so much? Why do we want to hug them and kiss them and play with them? Why do we want to take care of them and make sure they are happy and healthy? The answer is simple: because they are cute! But what makes a dog cute? Is it their big eyes, their floppy ears, their wagging tail, or something else? In this article, we will explore the science behind why dogs are so cute and why we love them so much.\n",
      "\n",
      "First, let's talk about the big eyes. Dogs have large eyes that are set far apart on their\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d72e48-e087-49dd-9f05-2c4577cd374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2e runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd907f7-6019-4b8b-be68-008b12d5456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-30:14:46:14,569 INFO     [__init__.py:29] Skipping import of cpp extensions\n",
      "Namespace(repo_id='meta-llama/Meta-Llama-3-8B', tasks=['hellaswag'], limit=None, precision=torch.bfloat16, device='cuda', quantization='None', sparsity='None', compile=True, batch_size=1, max_length=None)\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "2024-07-30:14:46:31,479 WARNING  [huggingface.py:122] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-07-30:14:46:31,511 WARNING  [huggingface.py:328] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-07-30:14:46:31,567 INFO     [__init__.py:491] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n",
      "2024-07-30:14:46:45,332 INFO     [task.py:423] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:03<00:00, 2510.71it/s]\n",
      "2024-07-30:14:46:50,668 INFO     [evaluator.py:457] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                 | 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jessecai/local/ao/scripts/hf_eval.py\", line 112, in <module>\n",
      "    run_evaluation(args.repo_id, args.tasks, args.limit, args.device, args.precision, args.quantization, args.sparsity, args.compile, args.batch_size, args.max_length)\n",
      "  File \"/home/jessecai/local/ao/scripts/hf_eval.py\", line 82, in run_evaluation\n",
      "    result = evaluate(\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/utils.py\", line 397, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/evaluator.py\", line 468, in evaluate\n",
      "    resps = getattr(lm, reqtype)(cloned_reqs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/api/model.py\", line 375, in loglikelihood\n",
      "    return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/models/huggingface.py\", line 990, in _loglikelihood_tokens\n",
      "    for chunk in chunks:\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/models/utils.py\", line 433, in get_batched\n",
      "    yield from batch\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/models/utils.py\", line 616, in get_chunks\n",
      "    if len(arr) == (fn(i, _iter) if fn else n):\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/models/huggingface.py\", line 923, in _batch_scheduler\n",
      "    self.batch_sizes[sched] = self._detect_batch_size(n_reordered_requests, pos)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/models/huggingface.py\", line 697, in _detect_batch_size\n",
      "    batch_size = forward_batch()\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/accelerate/utils/memory.py\", line 146, in decorator\n",
      "    return function(batch_size, *args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/models/huggingface.py\", line 692, in forward_batch\n",
      "    out = F.log_softmax(self._model_call(test_batch, **call_kwargs), dim=-1)  # noqa: F841\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/lm_eval/models/huggingface.py\", line 801, in _model_call\n",
      "    return self.model(inps).logits\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1735, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1746, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 462, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1735, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1746, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1182, in __call__\n",
      "    return self._torchdynamo_orig_callable(\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 506, in __call__\n",
      "    return _compile(\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 862, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 272, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_utils_internal.py\", line 85, in wrapper_function\n",
      "    return StrobelightCompileTimeProfiler.profile_compile_time(\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 680, in compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 201, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 622, in transform\n",
      "    tracer.run()\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2657, in run\n",
      "    super().run()\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 925, in run\n",
      "    while self.step():\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 837, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 529, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1605, in CALL_FUNCTION_KW\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 775, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py\", line 440, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2872, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2988, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 925, in run\n",
      "    while self.step():\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 837, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 529, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1593, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 775, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 370, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 309, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 106, in call_function\n",
      "    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2872, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2988, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 925, in run\n",
      "    while self.step():\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 837, in step\n",
      "    self.dispatch_table[inst.opcode](self, inst)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 529, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1532, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 775, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 757, in call_function\n",
      "    return self.obj.call_method(tx, self.name, args, kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/variables/misc.py\", line 1202, in call_method\n",
      "    unimplemented(\"Logger not supported for non-export cases\")\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 271, in unimplemented\n",
      "    raise Unsupported(msg, case_name=case_name)\n",
      "torch._dynamo.exc.Unsupported: Logger not supported for non-export cases\n",
      "\n",
      "from user code:\n",
      "   File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1145, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1746, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 905, in forward\n",
      "    logger.warning_once(\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "Running loglikelihood requests:   0%|                 | 0/40168 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/hf_eval.py --tasks hellaswag --compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1e92dd-d457-4cb7-80d3-d7b023590901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-30:14:03:14,449 INFO     [__init__.py:29] Skipping import of cpp extensions\n",
      "Namespace(repo_id='nm-testing/SparseLlama-3-8B-pruned_50.2of4', tasks=['hellaswag'], limit=None, precision=torch.bfloat16, device='cuda', quantization='None', sparsity='semi_sparse_mlp_only', compile=False, batch_size=1, max_length=None)\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.26it/s]\n",
      "/home/jessecai/local/miniconda3/envs/huggingface-nm-pt/lib/python3.10/site-packages/torch/sparse/semi_structured.py:113: UserWarning: The PyTorch API of SparseSemiStructuredTensor is in prototype stage and will change in the near future. Please open a Github issue for features requests and see our documentation on the torch.sparse module for further information about the project.\n",
      "  warnings.warn(\n",
      "2024-07-30:14:03:30,297 WARNING  [huggingface.py:122] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-07-30:14:03:30,333 WARNING  [huggingface.py:328] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "2024-07-30:14:03:30,396 INFO     [__init__.py:491] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n",
      "2024-07-30:14:03:44,357 INFO     [task.py:423] Building contexts for hellaswag on rank 0...\n",
      "100%|███████████████████████████████████| 10042/10042 [00:04<00:00, 2336.94it/s]\n",
      "2024-07-30:14:03:49,770 INFO     [evaluator.py:457] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                 | 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Determined largest batch size: 32\n",
      "Running loglikelihood requests: 100%|████| 40168/40168 [04:34<00:00, 146.48it/s]\n",
      "+-----------+-------------------------------------+\n",
      "| Task      | Metrics                             |\n",
      "+===========+=====================================+\n",
      "| hellaswag | +----------------------+----------+ |\n",
      "|           | | acc,none             | 0.58813  | |\n",
      "|           | +----------------------+----------+ |\n",
      "|           | | acc_norm,none        | 0.778829 | |\n",
      "|           | +----------------------+----------+ |\n",
      "|           | | acc_norm_stderr,none | 0.004142 | |\n",
      "|           | +----------------------+----------+ |\n",
      "|           | | acc_stderr,none      | 0.004912 | |\n",
      "|           | +----------------------+----------+ |\n",
      "+-----------+-------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!python scripts/hf_eval.py --sparsity semi_sparse_mlp_only --repo_id \"nm-testing/SparseLlama-3-8B-pruned_50.2of4\" --tasks hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f49163-52c3-4a4f-a4f3-db849e85dca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
